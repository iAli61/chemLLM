{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fcc61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured - warnings suppressed!\n"
     ]
    }
   ],
   "source": [
    "# Suppress common warnings and configure environment\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN if causing issues\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Suppress tokenizer warnings\n",
    "\n",
    "# Set logging levels\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"Environment configured - warnings suppressed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99926a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f6447",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b4e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibina/repo/Training_LLM/chemLLM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '641e99912bb8b89e81d099b3', 'name': 'iAli61', 'fullname': 'Ali Bina', 'isPro': False, 'avatarUrl': '/avatars/dff6c063bbc2be75d628727692ff4092.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'HF_TOKEN', 'role': 'fineGrained', 'createdAt': '2025-06-29T15:29:08.821Z', 'fineGrained': {'canReadGatedRepos': True, 'global': [], 'scoped': [{'entity': {'_id': '641e99912bb8b89e81d099b3', 'type': 'user', 'name': 'iAli61'}, 'permissions': ['repo.content.read', 'repo.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "user_info = api.whoami(token=os.getenv(\"HF_TOKEN\"))\n",
    "print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd7abe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 14:50:25.447519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751719825.629149  270515 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751719825.671449  270515 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751719826.108366  270515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751719826.108713  270515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751719826.108727  270515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751719826.108731  270515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e1c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c329a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n",
      " -> Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n",
      "model -> Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens -> Embedding(151936, 1024)\n",
      "model.layers -> ModuleList(\n",
      "  (0-27): 28 x Qwen3DecoderLayer(\n",
      "    (self_attn): Qwen3Attention(\n",
      "      (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    )\n",
      "    (mlp): Qwen3MLP(\n",
      "      (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "model.layers.0 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.0.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.0.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.0.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.0.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.0.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.0.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.0.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.0.mlp.act_fn -> SiLU()\n",
      "model.layers.0.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.0.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.1 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.1.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.1.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.1.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.1.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.1.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.1.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.1.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.1.mlp.act_fn -> SiLU()\n",
      "model.layers.1.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.1.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.2 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.2.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.2.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.2.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.2.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.2.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.2.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.2.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.2.mlp.act_fn -> SiLU()\n",
      "model.layers.2.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.2.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.3 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.3.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.3.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.3.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.3.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.3.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.3.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.3.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.3.mlp.act_fn -> SiLU()\n",
      "model.layers.3.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.3.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.4 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.4.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.4.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.4.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.4.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.4.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.4.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.4.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.4.mlp.act_fn -> SiLU()\n",
      "model.layers.4.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.4.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.5 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.5.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.5.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.5.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.5.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.5.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.5.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.5.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.5.mlp.act_fn -> SiLU()\n",
      "model.layers.5.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.5.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.6 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.6.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.6.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.6.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.6.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.6.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.6.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.6.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.6.mlp.act_fn -> SiLU()\n",
      "model.layers.6.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.6.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.7 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.7.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.7.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.7.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.7.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.7.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.7.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.7.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.7.mlp.act_fn -> SiLU()\n",
      "model.layers.7.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.7.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.8 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.8.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.8.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.8.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.8.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.8.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.8.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.8.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.8.mlp.act_fn -> SiLU()\n",
      "model.layers.8.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.8.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.9 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.9.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.9.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.9.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.9.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.9.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.9.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.9.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.9.mlp.act_fn -> SiLU()\n",
      "model.layers.9.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.9.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.10 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.10.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.10.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.10.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.10.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.10.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.10.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.10.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.10.mlp.act_fn -> SiLU()\n",
      "model.layers.10.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.10.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.11 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.11.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.11.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.11.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.11.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.11.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.11.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.11.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.11.mlp.act_fn -> SiLU()\n",
      "model.layers.11.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.11.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.12 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.12.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.12.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.12.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.12.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.12.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.12.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.12.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.12.mlp.act_fn -> SiLU()\n",
      "model.layers.12.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.12.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.13 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.13.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.13.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.13.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.13.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.13.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.13.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.13.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.13.mlp.act_fn -> SiLU()\n",
      "model.layers.13.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.13.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.14 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.14.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.14.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.14.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.14.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.14.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.14.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.14.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.14.mlp.act_fn -> SiLU()\n",
      "model.layers.14.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.14.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.15 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.15.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.15.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.15.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.15.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.15.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.15.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.15.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.15.mlp.act_fn -> SiLU()\n",
      "model.layers.15.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.15.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.16 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.16.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.16.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.16.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.16.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.16.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.16.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.16.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.16.mlp.act_fn -> SiLU()\n",
      "model.layers.16.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.16.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.17 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.17.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.17.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.17.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.17.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.17.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.17.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.17.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.17.mlp.act_fn -> SiLU()\n",
      "model.layers.17.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.17.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.18 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.18.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.18.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.18.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.18.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.18.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.18.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.18.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.18.mlp.act_fn -> SiLU()\n",
      "model.layers.18.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.18.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.19 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.19.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.19.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.19.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.19.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.19.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.19.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.19.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.19.mlp.act_fn -> SiLU()\n",
      "model.layers.19.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.19.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.20 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.20.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.20.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.20.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.20.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.20.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.20.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.20.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.20.mlp.act_fn -> SiLU()\n",
      "model.layers.20.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.20.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.21 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.21.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.21.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.21.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.21.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.21.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.21.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.21.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.21.mlp.act_fn -> SiLU()\n",
      "model.layers.21.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.21.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.22 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.22.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.22.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.22.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.22.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.22.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.22.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.22.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.22.mlp.act_fn -> SiLU()\n",
      "model.layers.22.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.22.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.23 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.23.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.23.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.23.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.23.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.23.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.23.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.23.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.23.mlp.act_fn -> SiLU()\n",
      "model.layers.23.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.23.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.24 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.24.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.24.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.24.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.24.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.24.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.24.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.24.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.24.mlp.act_fn -> SiLU()\n",
      "model.layers.24.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.24.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.25 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.25.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.25.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.25.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.25.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.25.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.25.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.25.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.25.mlp.act_fn -> SiLU()\n",
      "model.layers.25.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.25.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.26 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.26.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.26.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.26.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.26.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.26.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.26.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.26.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.26.mlp.act_fn -> SiLU()\n",
      "model.layers.26.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.26.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.27 -> Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "model.layers.27.self_attn -> Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "model.layers.27.self_attn.q_proj -> Linear(in_features=1024, out_features=2048, bias=False)\n",
      "model.layers.27.self_attn.k_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj -> Linear(in_features=1024, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj -> Linear(in_features=2048, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.q_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.27.self_attn.k_norm -> Qwen3RMSNorm((128,), eps=1e-06)\n",
      "model.layers.27.mlp -> Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.27.mlp.up_proj -> Linear(in_features=1024, out_features=3072, bias=False)\n",
      "model.layers.27.mlp.down_proj -> Linear(in_features=3072, out_features=1024, bias=False)\n",
      "model.layers.27.mlp.act_fn -> SiLU()\n",
      "model.layers.27.input_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.layers.27.post_attention_layernorm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.norm -> Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "model.rotary_emb -> Qwen3RotaryEmbedding()\n",
      "lm_head -> Linear(in_features=1024, out_features=151936, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# print the top‐level Qwen3ForCausalLM structure\n",
    "print(model)\n",
    "\n",
    "# or inspect its submodules in detail\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"{name} -> {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c333a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=2048, bias=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# model.layers.26.self_attn\n",
    "model.model.layers[26].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a56458b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_ID = \"iAli61/chempile-education-dedup-dedup\"\n",
    "\n",
    "dataset = load_dataset(DATASET_ID, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f1ef40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'text'],\n",
       "        num_rows: 28664\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31ea84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[43132,    13, 27131,   378,   262,  4894,   286,  9978]])\n",
      "Target IDs: tensor([[   13, 27131,   378,   262,  4894,   286,  9978,    11]])\n",
      "Input IDs shape: torch.Size([1, 8])\n",
      "Target IDs shape: torch.Size([1, 8])\n",
      "dataset length: 28664\n",
      "dataloader length: 2339\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "COLUMN = \"text\"  # Column to be used for deduplication\n",
    "\n",
    "class CHEMPILE_DS_V1(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=256, stride=256):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # add \"<|endoftext|>\" to the end of each text\n",
    "        self.texts = \" \".join([text + \"<|endoftext|>\" for text in dataset[COLUMN]][:10])\n",
    "        self.token_ids = tokenizer.encode(self.texts, allowed_special={\"<|endoftext|>\"})\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # using a sliding window approach to create input-target pairs\n",
    "        for i in range(0, len(self.token_ids) - self.max_length, stride):\n",
    "            input_ids = self.token_ids[i:i + self.max_length]\n",
    "            target_ids = self.token_ids[i + 1:i + self.max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_ids))\n",
    "            self.target_ids.append(torch.tensor(target_ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      tokenizer, \n",
    "                      batch_size=8, \n",
    "                      max_length=256, \n",
    "                      stride=256,\n",
    "                      drop_last=True,\n",
    "                      shuffle=True\n",
    "                      ):\n",
    "\n",
    "    chempile_ds = CHEMPILE_DS_V1(dataset, tokenizer, max_length=max_length, stride=stride)\n",
    "\n",
    "    return DataLoader(chempile_ds, \n",
    "                      batch_size=batch_size, \n",
    "                      drop_last=drop_last,\n",
    "                      shuffle=shuffle,\n",
    "                      )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataloader = create_dataloader(dataset, tokenizer, batch_size=1, max_length=8, stride=8)\n",
    "\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "input_ids, target_ids = next(dataiter)\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Target IDs: {target_ids}\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Target IDs shape: {target_ids.shape}\")\n",
    "\n",
    "print(f\"dataset length: {len(dataset)}\")\n",
    "print(f\"dataloader length: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9222f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61eb0c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': ['Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Chemical_Thermodynamics_(Supplement_to_Shepherd_et_al.)/04%3A_Fundamental_2_-_Counting_Configurations/4.04%3A_Applying_the_Laws_of_Probability'],\n",
       " 'text': ['The laws of probability apply to events that are independent. If the result of one trial depends on the result of another trial, we may still be able to use the laws of probability. However, to do so, we must know the nature of the interdependence. If the activity associated with event C precedes the activity associated with event D , the probability of D may depend on whether C occurs. Suppose that the first activity is tossing a coin and that the second activity is drawing a card from a deck; however, the deck we use depends on whether the coin comes up heads or tails. If the coin is heads, we draw a card from an ordinary deck; if the coin is tails, we draw a coin from a deck with the face cards removed. Now we ask about the probability of drawing an ace. If the coin is heads, the probability of drawing an ace is \\\\({4}/{52}={1}/{13}\\\\). If the coin is tails, the probability of drawing an ace is \\\\({4}/{40}={1}/{10}\\\\). The combination coin is heads and card is ace has probability: \\\\(\\\\left({1}/{2}\\\\right)\\\\left({1}/{13}\\\\right)={1}/{26}\\\\). The combination coin is tails and card is ace has probability \\\\(\\\\left({1}/{2}\\\\right)\\\\left({1}/{10}\\\\right)={1}/{20}\\\\). In this case, the probability of drawing an ace depends on the modification we make to the deck based on the outcome of the coin toss. Applying the laws of probability is straightforward. An example that illustrates the application of these laws in a transparent way is provided by villages First, Second, Third, and Fourth, which are separated by rivers. (See Figure 1.) Bridges \\\\(1\\\\), \\\\(2\\\\), and \\\\(3\\\\) span the river between First and Second. Bridges \\\\(a\\\\) and \\\\(b\\\\) span the river between Second and Third. Bridges \\\\(A\\\\), \\\\(B\\\\), \\\\(C\\\\), and \\\\(D\\\\) span the river between Third and Fourth. A traveler from First to Fourth who is free to take any route he pleases has a choice from among \\\\(3\\\\times 2\\\\times 4=24\\\\) possible combinations. Let us consider the probabilities associated with various events: There are 24 possible routes. If a traveler chooses his route at random, the probability that he will take any particular route is \\\\({1}/{24}\\\\). This illustrates our assumption that each event in a set of \\\\(N\\\\) exhaustive and mutually exclusive events occurs with probability \\\\({1}/{N}\\\\). If he chooses a route at random, the probability that he goes from First to Second by either bridge \\\\(1\\\\) or bridge \\\\(2\\\\) is \\\\(P\\\\left(1\\\\right)+P\\\\left(2\\\\right)=\\\\ {1}/{3}+{1}/{3}={2}/{3}\\\\). This illustrates the calculation of the probability of alternative events. The probability of the particular route \\\\(2\\\\to a\\\\to C\\\\) is \\\\(P\\\\left(2\\\\right)\\\\times P\\\\left(a\\\\right)\\\\times P\\\\left(C\\\\right)=\\\\left({1}/{3}\\\\right)\\\\left({1}/{2}\\\\right)\\\\left({1}/{4}\\\\right)={1}/{24}\\\\), and we calculate the same probability for any other route from First to Fourth. This illustrates the calculation of the probability of a compound event. If he crosses bridge \\\\(1\\\\), the probability that his route will be \\\\(2\\\\to a\\\\to C\\\\) is zero, of course. The probability of an event that has already occurred is 1, and the probability of any alternative is zero. If he crosses bridge \\\\(1,\\\\) \\\\(P\\\\left(1\\\\right)=1\\\\), and \\\\(P\\\\left(2\\\\right)=P\\\\left(3\\\\right)=0\\\\). Given that a traveler has used bridge \\\\(1\\\\), the probability of the route \\\\(1\\\\to a\\\\to C\\\\) becomes the probability of path \\\\(a\\\\to C\\\\), which is \\\\(P\\\\left(a\\\\right)\\\\times P\\\\left(C\\\\right)=\\\\left({1}/{2}\\\\right)\\\\left({1}/{4}\\\\right)={1}/{8}\\\\). Since \\\\(P\\\\left(1\\\\right)=1\\\\), the probability of the compound event \\\\(1\\\\to a\\\\to C\\\\) is the probability of the compound event \\\\(a\\\\to C\\\\). The outcomes of rolling dice, rolling provide more illustrations. If we roll two dice, we can classify the possible outcomes according to the sums of the outcomes for the individual dice. There are thirty-six possible outcomes. They are displayed in Table 1. Table 1: Outcomes from tossing two dice 0 1 2 3 4 5 6 7 NaN Outcome for first die Outcome for first die Outcome for first die Outcome for first die Outcome for first die Outcome for first die Outcome for first die Outcome for second die NaN 1 2 3 4 5 6 Outcome for second die 1 2 3 4 5 6 7 Outcome for second die 2 3 4 5 6 7 8 Outcome for second die 3 4 5 6 7 8 9 Outcome for second die 4 5 6 7 8 9 10 Outcome for second die 5 6 7 8 9 10 11 Outcome for second die 6 7 8 9 10 11 12 Let us consider the probabilities associated with various dice-throwing events: The probability of any given outcome, say the first die shows \\\\(2\\\\) and the second die shows \\\\(3\\\\), is \\\\({1}/{36}\\\\). Since the probability that the first die shows \\\\(3\\\\) while the second die shows \\\\(2\\\\) is also \\\\({1}/{36}\\\\), the probability that one die shows \\\\(2\\\\) and the other shows \\\\(3\\\\) is \\\\[P\\\\left(3\\\\right)\\\\times P\\\\left(2\\\\right)+P\\\\left(2\\\\right)\\\\times P\\\\left(3\\\\right) =\\\\left({1}/{36}\\\\right)+\\\\left({1}/{36}\\\\right) ={1}/{18}. \\\\nonumber \\\\] Four different outcomes correspond to the event that the score is \\\\(5\\\\). Therefore, the probability of rolling \\\\(5\\\\) is \\\\[P\\\\left(1\\\\right)\\\\times P\\\\left(4\\\\right)+P\\\\left(2\\\\right)\\\\times P\\\\left(3\\\\right) +P\\\\left(3\\\\right)\\\\times P\\\\left(2\\\\right)+P\\\\left(4\\\\right)\\\\times P\\\\left(1\\\\right) ={1}/{9} \\\\nonumber \\\\] The probability of rolling a score of three or less is the probability of rolling \\\\(2\\\\), plus the probability of rolling \\\\(3\\\\) which is \\\\(\\\\left({1}/{36}\\\\right)+\\\\left({2}/{36}\\\\right)={3}/{36}={1}/{12}\\\\) Suppose we roll the dice one at a time and that the first die shows \\\\(2\\\\). The probability of rolling \\\\(7\\\\) when the second die is thrown is now \\\\({1}/{6}\\\\), because only rolling a \\\\(5\\\\) can make the score 7, and there is a probability of \\\\({1}/{6}\\\\) that a \\\\(5\\\\) will come up when the second die is thrown. Suppose the first die is red and the second die is green. The probability that the red die comes up \\\\(2\\\\) and the green die comes up \\\\(3\\\\) is \\\\(\\\\left({1}/{6}\\\\right)\\\\left({1}/{6}\\\\right)={1}/{36}\\\\). Above we looked at the number of outcomes associated with a score of \\\\(3\\\\) to find that the probability of this event is \\\\({1}/{18}\\\\). We can use another argument to get this result. The probability that two dice roll a score of three is equal to the probability that the first die shows \\\\(1\\\\) or \\\\(2\\\\) times the probability that the second die shows whatever score is necessary to make the total equal to three. This is: \\\\[\\\\begin{align*} P\\\\left(first\\\\ die\\\\ shows\\\\ 1\\\\ or\\\\ 2\\\\right)\\\\times \\\\left({1}/{6}\\\\right) &= \\\\left[\\\\left({1}/{6}\\\\right)+\\\\left({1}/{6}\\\\right)\\\\right]\\\\times {1}/{6} \\\\\\\\[4pt] &={2}/{36} \\\\\\\\[4pt]& ={1}/{18} \\\\end{align*} \\\\nonumber \\\\] Application of the laws of probability is frequently made easier by recognizing a simple restatement of the requirement that events be mutually exclusive. In a given trial, either an event occurs or it does not. Let the probability that an event A occurs be \\\\(P\\\\left(A\\\\right)\\\\). Let the probability that event A does not occur be \\\\(P\\\\left(\\\\sim A\\\\right)\\\\). Since in any given trial, the outcome must belong either to event A or to event \\\\(\\\\sim A\\\\), we have \\\\[P\\\\left(A\\\\right)+P\\\\left(\\\\sim A\\\\right)=1 \\\\nonumber \\\\] For example, if the probability of success in a single trial is \\\\({2}/{3}\\\\), the probability of failure is \\\\({1}/{3}\\\\). If we consider the outcomes of two successive trials, we can group them into four events. Event SS: First trial is a success; second trial is a success. Event SF: First trial is a success; second trial is a failure. Event FS: First trial is a failure; second trial is a success. Event FF: First trial is a failure; second trial is a failure. Using the laws of probability, we have \\\\[ \\\\begin{align*} 1 &=P\\\\left(Event\\\\ SS\\\\right)+P\\\\left(Event\\\\ SF\\\\right)+P\\\\left(Event\\\\ FS\\\\right)+\\\\ P(Event\\\\ FF) \\\\\\\\[4pt] &=P_1\\\\left(S\\\\right)\\\\times P_2\\\\left(S\\\\right)+P_1\\\\left(S\\\\right)\\\\times P_2\\\\left(F\\\\right) +P_1(F)\\\\times P_2(S)+P_1(F)\\\\times P_2(F) \\\\end{align*} \\\\nonumber \\\\] where \\\\(P_1\\\\left(X\\\\right)\\\\) and \\\\(P_2\\\\left(X\\\\right)\\\\) are the probability of event \\\\(X\\\\) in the first and second trials, respectively. This situation can be mapped onto a simple diagram. We represent the possible outcomes of the first trial by line segments on one side of a unit square \\\\(P_1\\\\left(S\\\\right)+P_1\\\\left(F\\\\right)=1\\\\). We represent the outcomes of the second trial by line segments along an adjoining side of the unit square. The four possible events are now represented by the areas of four mutually exclusive and exhaustive portions of the unit square as shown in Figure 2.']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n",
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201a6dc",
   "metadata": {},
   "source": [
    "## Multihead attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bca41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d67c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MHA, self).__init__()\n",
    "        # check if number of heads divides embedding dimension\n",
    "        if config['emb_dim'] % config['n_heads'] != 0:\n",
    "            raise ValueError(\"Embedding dimension must be divisible by number of heads.\")\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.head_dim = config['emb_dim'] // self.n_heads\n",
    "        self.config = config\n",
    "        # dimensions [batch_size, sequence_length, 3 * emb_dim for q, k, v]\n",
    "        self.qkv = nn.Linear(config['emb_dim'], config['emb_dim'] * 3, bias=config['qkv_bias'])\n",
    "        self.attn_drop = nn.Dropout(config['drop_rate'])\n",
    "        self.proj = nn.Linear(config['emb_dim'], config['emb_dim'])\n",
    "        self.proj_drop = nn.Dropout(config['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        # B: batch size, T: sequence length, C: embedding dimension\n",
    "        # (B, T, 3 * C) -> (B, T, 3, n_heads, head_dim) \n",
    "        # where n_heads = config['n_heads'] and head_dim = C // n_heads\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.config['n_heads'], self.head_dim)\n",
    "        # (B, T, 3, n_heads, head_dim) -> (3, B, n_heads, T, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        use_dropout = 0. if self.training else self.config['drop_rate']\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, \n",
    "            attn_mask=None,\n",
    "            dropout_p=use_dropout, \n",
    "            is_causal=True\n",
    "        )\n",
    "\n",
    "        # (B, n_heads, T, head_dim) -> (B, T, n_heads * head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(B, T, self.config['emb_dim'])\n",
    "        # (B, T, emb_dim) -> (B, T, emb_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd36755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.config = config\n",
    "        self.mha = MHA(config)\n",
    "        self.ln1 = nn.LayerNorm(config['emb_dim'])\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], config['emb_dim'] * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config['emb_dim'] * 4, config['emb_dim'])\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config['emb_dim'])\n",
    "        self.drop = nn.Dropout(config['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head attention\n",
    "        x = x + self.drop(self.mha(self.ln1(x)))\n",
    "        # Feed-forward network\n",
    "        x = x + self.drop(self.ffn(self.ln2(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98bac7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer block time: 1.738435 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x = torch.randn(32, 1024, GPT_CONFIG_124M['emb_dim']).cuda()  # Example input tensor\n",
    "# clean up gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "# Measure time for custom Transformer block\n",
    "transformer_block = TransformerBlock(GPT_CONFIG_124M).cuda()  # Move to GPU if available\n",
    "start_time = time.time()\n",
    "output_transformer = transformer_block(x)\n",
    "transformer_time = time.time() - start_time \n",
    "\n",
    "print(f\"Transformer block time: {transformer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "387bf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_embedding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.dropout_emb = nn.Dropout(config['drop_rate'])\n",
    "        # Using nn.Sequential for transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config['n_layers'])]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(config['emb_dim'])\n",
    "        self.head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        # x: (B, T) where B is batch size and T is sequence length\n",
    "        # (B, T) -> (B, T, emb_dim)\n",
    "        x = self.embedding(x) + self.positional_embedding(torch.arange(T, device=x.device))\n",
    "        x = self.dropout_emb(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        # Final layer normalization and linear projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e54cb630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='cuda:0')\n",
      "torch.Size([2, 4])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0).cuda()\n",
    "print(batch)\n",
    "print(batch.shape)  # Should be (batch_size, sequence_length)\n",
    "print(batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2baf89df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6109, 3626, 6100, 345]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "318b1a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1841,  0.9427, -0.6268,  ...,  0.4099,  0.6749, -0.1008],\n",
       "         [-0.2650,  1.4717, -0.9401,  ...,  0.2010,  0.2585, -0.3980],\n",
       "         [-0.2078, -0.1370,  0.4248,  ...,  0.2952,  0.2042,  0.2585],\n",
       "         [-1.4372,  0.2441,  0.2246,  ..., -0.0817, -0.0642,  0.4755]],\n",
       "\n",
       "        [[ 0.1748,  1.0229, -0.9393,  ..., -0.0309,  1.0036, -0.5670],\n",
       "         [ 0.4112,  0.9963,  0.5240,  ..., -0.3183,  0.9374,  0.0583],\n",
       "         [-0.8739, -0.4290, -0.2994,  ..., -0.3820,  0.3080, -0.0327],\n",
       "         [-1.0862,  0.4183, -0.2846,  ..., -0.9729,  0.0065,  0.4931]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Measure time for custom Transformer block\n",
    "GPT2_model = GPT2(GPT_CONFIG_124M).cuda()\n",
    "\n",
    "output_gpt2 = GPT2_model(batch)\n",
    "\n",
    "print(output_gpt2.shape)  # Should be (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "output_gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9d1a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in GPT2_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d307030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_greedy_V1(model, prompt, tokenizer, context_length=1024, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT model.\n",
    "        prompt: The input text prompt.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        context_length: The maximum context length for the model.\n",
    "        max_new_tokens: The maximum number of new tokens to generate.\n",
    "        \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "    # check if the mode is not gpu, move it to gpu\n",
    "    if not next(model.parameters()).is_cuda:\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).cuda()\n",
    "\n",
    "    generated_ids = input_ids[:, -(context_length - 1):] # Keep the last context_length - 1 tokens\n",
    "    generated_ids = generated_ids.cuda()  # Move to GPU if available\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(generated_ids)\n",
    "            next_token_logits = outputs[:, -1, :]  # Get logits for the last token\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # Greedy decoding\n",
    "            \n",
    "            generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
    "            \n",
    "            if generated_ids.size(1) >= context_length:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1416716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves youabl labou knives dungeonsative mother resulting Joyce continuous regexahovez importance tast Needless wrestling revive weLeague Highland Teresa ssh elegance shaped clitor bowel Fleming metast wildProgramlas Introductionverting Phase Reach eviction estimation CommercialIFIED TFentimes differingcus CosponsorsBeautApr Nexeps Submitgang\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Every effort moves you\"\n",
    "generated_text = text_generation_greedy_V1(GPT2_model, prompt, tokenizer,\n",
    "                                           context_length=GPT_CONFIG_124M['context_length'],\n",
    "                                           max_new_tokens=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "468cfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters and flops\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "def get_model_parameters_and_flops(model, input_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    return params, macs*2\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "input_tensor = torch.randint(0, GPT_CONFIG_124M['vocab_size'], (2, GPT_CONFIG_124M['context_length'])).cuda()\n",
    "\n",
    "# for model_name, config in model_configs.items():\n",
    "#     CONFIG = GPT_CONFIG_124M.copy()\n",
    "#     CONFIG.update(config)  # Update with specific model configuration\n",
    "#     model = GPT2(CONFIG).cuda()\n",
    "#     params, flops = get_model_parameters_and_flops(model.bfloat16(), input_tensor)\n",
    "#     print(f\"Number of parameters: {params:.1e} M, FLOPs: {flops:.1e} G for {model_name}\")\n",
    "\n",
    "\n",
    "#     del model  # Delete the model to free up memory\n",
    "#     torch.cuda.empty_cache()  # Clear GPU memory after each model to avoid OOM errors\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692b937",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d30ee7",
   "metadata": {},
   "source": [
    "### Configurations and Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55fd1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT2(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd129d79",
   "metadata": {},
   "source": [
    "### Train/validation data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16b31601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text'],\n",
       "    num_rows: 28664\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27625d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 28664\n",
      "Training data size: 25797\n",
      "Validation data size: 2867\n",
      "Number of training batches: 32\n",
      "Number of validation batches: 25\n"
     ]
    }
   ],
   "source": [
    "# Train/validation ratio\n",
    "train_val_split = dataset.train_test_split(test_size=0.1, seed=123)\n",
    "val_dataset = train_val_split['test']\n",
    "train_dataset = train_val_split['train']\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Training data size: {len(train_val_dataset)}\")\n",
    "print(f\"Validation data size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    batch_size=2,\n",
    "    max_length=256,\n",
    "    stride=256,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_dataset,\n",
    "    tokenizer,\n",
    "    batch_size=2,\n",
    "    max_length=256,\n",
    "    stride=256,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8d33668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27579694",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ba8e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "155c7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n",
      "Training loss: 10.993945519129435\n",
      "Perplexity: 59512.71\n",
      "Validation loss: 10.981866778749408\n",
      "Perplexity: 58798.22\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(train_loss)):.2f}\")\n",
    "print(\"Validation loss:\", val_loss)\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(val_loss)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054212ef",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9c2daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.positional_embedding.weight.shape[0]\n",
    "    with torch.no_grad():\n",
    "        generated_text = text_generation_greedy_V1(\n",
    "            model=model, \n",
    "            prompt=start_context,\n",
    "            tokenizer=tokenizer,\n",
    "            context_length=context_size,\n",
    "            max_new_tokens=50   \n",
    "        )\n",
    "    print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3962f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.385, Val loss 10.533\n",
      "Ep 1 (Step 000005): Train loss 8.875, Val loss 9.240\n",
      "Ep 1 (Step 000010): Train loss 8.062, Val loss 8.619\n",
      "Ep 1 (Step 000015): Train loss 7.435, Val loss 8.460\n",
      "Ep 1 (Step 000020): Train loss 7.128, Val loss 8.594\n",
      "Ep 1 (Step 000025): Train loss 7.021, Val loss 8.636\n",
      "Ep 1 (Step 000030): Train loss 6.735, Val loss 8.493\n",
      "Generated text: Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000035): Train loss 6.508, Val loss 8.449\n",
      "Ep 2 (Step 000040): Train loss 6.627, Val loss 8.438\n",
      "Ep 2 (Step 000045): Train loss 6.564, Val loss 8.637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m0.0004\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m      6\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     12\u001b[39m     optimizer.zero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     loss.backward() \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[32m     15\u001b[39m     optimizer.step() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     input_batch, target_batch = \u001b[43minput_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, target_batch.to(device)\n\u001b[32m      3\u001b[39m     logits = model(input_batch)\n\u001b[32m      4\u001b[39m     loss = torch.nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPT2(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bdd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebeae36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
